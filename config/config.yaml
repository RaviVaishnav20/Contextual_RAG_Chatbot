# Configuration file for Contextual RAG ChatBot

# Directory paths
directories:
  resources: "/Users/ravi/Contextual_RAG_ChatBot/resources"
  markdown_artifacts: "artifacts/markdown_artifacts"
  metadata_chunk_file: "artifacts/metadata_chunk.json"
  metadata_context_chunk_file: "artifacts/metadata_context_chunk.json"
  file_hash_cache: "artifacts/file_hash_cache.json"

# Chunking configuration
chunking:
  model:
    provider: "ollama"
    model_name: "llama3.1:8b"
    fallback_provider: "gemini"
    fallback_model_name: "gemini-2.5-flash"
  parameters:
    chunk_size: 256
    chunk_overlap: 50
    word_limit: 512

# Contextual retrieval configuration
# contextual_retrieval:
#   primary:
#     provider: "ollama"
#     model_name: "llama3.1:8b"
#   secondary:
#     provider: "gemini"
#     model_name: "gemini-2.5-flash"
#   tertiary:
#     provider: "groq"
#     model_name: "llama-3.3-70b-versatile"
#   parameters:
#     max_document_length: 450000
contextual_retrieval:
  primary:
    provider: "gemini"
    model_name: "gemini-2.5-flash"
  secondary:
    provider: "groq"
    model_name: "llama-3.3-70b-versatile"
  tertiary:
    provider: "bedrock"
    model_name: "claude"
  parameters:
    max_document_length: 450000  # Reduced for better performance
    timeout_seconds: 60  # Timeout for each LLM call
    batch_save_interval: 5  # Save progress every N chunks

reranker:
  provider: "ollama"
  model_name: "gemma3"
  fallback_provider: "gemini"
  fallback_model_name: "gemini-2.5-flash"
# Embedding configuration
embedding:
  provider: "ollama"
  model_name: "nomic-embed-text"
  # base_url: "http://localhost:11434"
  dimension: 768
  additional_kwargs:
    mirostat: 0
    timeout: 30

# Database configuration
database:
  host: "localhost"
  port: 5433
  database: "vector_db"
  user: "ravi"
  password: "password"
  table_name: "contextual_embedding"
  hnsw_kwargs:
    hnsw_m: 16
    hnsw_ef_construction: 64
    hnsw_ef_search: 40
    hnsw_dist_method: "vector_cosine_ops"

# RAG configuration
rag:
  model:
    provider: "ollama"
    model_name: "gemma3"
    fallback_provider: "gemini"
    fallback_model_name: "gemini-2.5-flash"
  parameters:
    similarity_top_k: 5
    temperature: 0.7
    # timeout: 30  # Add timeout for RAG queries

# Processing configuration
processing:
  export_type: "MARKDOWN"
  batch_size: 10
  enable_hash_checking: true
  parallel_processing: false  # Disable for stability

# CrewAI Multi-Agent Configuration
crewai:
  llm:
    provider: "ollama"
    model_name: "gemma3"


  # agents:
  #   general_agent:
  #     role: "General Knowledge Expert"
  #     model:
  #       provider: "ollama"
  #       model_name: "llama3:8b"
  #     temperature: 0.7
  #     max_tokens: 1000
    
  #   rag_agent:
  #     role: "Internal Knowledge Specialist"
  #     model:
  #       provider: "ollama"
  #       model_name: "llama3:8b"
  #     temperature: 0.3
  #     max_tokens: 1500
  #     search_top_k: 5
    
  #   web_agent:
  #     role: "Web Research Specialist"
  #     model:
  #       provider: "ollama"
  #       model_name: "llama3:8b"
  #     temperature: 0.5
  #     max_tokens: 1200
  #     search_results_limit: 10
    
  #   coordinator:
  #     role: "Query Coordinator"
  #     model:
  #       provider: "ollama"
  #       model_name: "llama3:8b"
  #     temperature: 0.4
  #     max_tokens: 2000
  
  # crew_settings:
  #   process: "hierarchical"
  #   verbose: true
  #   max_iterations: 3
  #   timeout_minutes: 10